#layers hidden dims learn.rate  1000xloss   comment
10      100         0.001                90
0       100         0.001               1.6e6       plateau
5       100         0.001               680         repeating "rough" patches, like shark finns, still learning exponentially
20      100         0.001                40         got below 0.4 in the first epoch
50      100         0.01                300         aborted, after initial fast learning got barely any learning over many epochs. Vanishing gradient?
30      100         0.01                330         stuck at .331, then rough patches then back at .331
30      100         0.001                34         learning better than at higher learn.rate
10      200         0.001                71         some fins, "under"exponential
20      50          0.001                27         many fins
20      50          0.0001               61         still some fins, slower learning
20      50          0.0005               18         a lot of fins, but best loss yet
Fix to loss calculation, loss below and above not comparable
20      50          0.0005                6         a lot of fins, but best loss yet
20      50          0.0002                6         less fins, but best loss yet, potential for more learning

Now with batches
#layers hidden dims learn.rate  batch   1e7xloss   comment
20      50          0.0002      256      <500       learns much faster, down to Loss 6000 within 9 Epochs, down to <1, but very noisy
from now on doing 100 epochs
20      50          0.0001      256      3550
20      50          0.001       256      8786
20      50          0.00001     256     40324       slow learning or flat, but more stable
20      50          0.0001      512     37318       slow learning, not flat, stable
20      50          0.00001     512    211018       very slow learning, similar stability
20      50          0.001       512     34770       faster learning, noisy, jumped back up
20      50          0.0001      128      6653       similarly noisy as 256, learns fast initially
20      50          0.00001     128     32690       looks same as for 0.0001 rate
20      50          0.001       128     14507       noisy and flatish

Fixed a bug in the loss function, losses after are not comparable to before

124k snps, 30 epochs
#layers hidden dims learn.rate  batch   1e7xtloss   1e7xvloss   snp-set     comment
10      1000        0.0001      256     140000      250000      0.1         in 5 epochs down to a plateau, then jumpy
20      50          0.0001      256     130000      200000      0.1         in 10 epochs down to a plateau, less jumpy than last trial
20      100         0.0001      256     80k-150k    150k-250k   0.1         very jumpy, also big difference between repeated run, maybe random sampling SNP is bad idea
20      50          0.0001      256     140k        240k        filtered    (trained on 100 epochs) Similar performance in vloss as the snp-fraction-set
20      200         0.00003     256     100k        250k        filtered    (trained on 60 epochs) Similar performance in vloss as the snp-fraction-set

The filtered (choosing ca 10k unlinked SNPs) compared to the fraction chosen (fraction of 10% at random) have similar loss.
Learning in the filtered case is much faster (10k SNPs vs 124k SNPs), but skipping the filtering is simpler and might be better with increasing data.